{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRIVIETNER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZQcP1oySdQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Connect to Google Drive if using google colab, otherwise skip.\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoYGKwlRSjp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Extract input to /data\n",
        "\"\"\"\n",
        "import tarfile\n",
        "def extracttar(fname):\n",
        "  if fname.endswith(\"tar.gz\"):\n",
        "      tar = tarfile.open(fname, \"r:gz\")\n",
        "      tar.extractall()\n",
        "      tar.close()\n",
        "  elif fname.endswith(\"tar\"):\n",
        "      tar = tarfile.open(fname, \"r:\")\n",
        "      tar.extractall()\n",
        "      tar.close()\n",
        "extracttar(\"/data/doc3.tar.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Clone VNCoreNLP and Facebook's FastText\n",
        "\"\"\"\n",
        "!git clone https://github.com/vncorenlp/VnCoreNLP\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!pip install .\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Kcfp4CSl5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Install Python Library\n",
        "\"\"\"\n",
        "!pip3 install vncorenlp\n",
        "!pip3 install underthesea\n",
        "!underthesea download tc_general\n",
        "!underthesea download tc_bank\n",
        "!pip install unidecode\n",
        "!pip3 install mechanize\n",
        "# !pip3 install fastText\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WELiyBfaSxDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Import Library\n",
        "\"\"\"\n",
        "import fastText\n",
        "import unidecode\n",
        "from underthesea import classify\n",
        "from vncorenlp import VnCoreNLP\n",
        "import pandas as pd\n",
        "import glob\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections import Counter \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJBmjSYqSzti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Annoate Document: Use VNCore NLP to segmentation, POS tagging, NER, classify and then dependency parsing raw data in data/mnt/doc/doc3/ and save as DataFrame in output/result_labeled_test.csv\n",
        "\"\"\"\n",
        "# annotator = VnCoreNLP(address=\"http://127.0.0.1\", port=58644) \n",
        "def annotateDocument(folderInput = 'data/mnt/doc/doc3/', folderOutput = 'output/'):\n",
        "  annotator = VnCoreNLP(\"VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g')\n",
        "  dataurl = glob.glob(folderInput + \"/*.txt\")\n",
        "  df = pd.DataFrame()\n",
        "  j = -1\n",
        "  for url in dataurl:\n",
        "    j = j + 1\n",
        "    text = open(url).read()\n",
        "    try:\n",
        "      s = annotator.annotate(text)\n",
        "      for i in range(len(s['sentences'])):\n",
        "        for x in [s['sentences'][i]]:\n",
        "          df_temp = pd.DataFrame(list(x))\n",
        "          if not ((df_temp.nerLabel == 'B-PER').any()):\n",
        "            continue\n",
        "          label = classify(text)\n",
        "          df_temp.insert(0, \"document\", j)\n",
        "          df_temp.insert(1,\"label\",label[0])\n",
        "          df = df.append(df_temp,True)   \n",
        "    except:\n",
        "      pass\n",
        "  df.to_csv(folderOutput + \"result_labeled.csv\", index = False, encoding='utf-8-sig')\n",
        "  return df\n",
        "\n",
        "annotateDocument()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CaJev0Ff3tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(file = r'/output/result_labeled.csv'):\n",
        "    \"\"\"\n",
        "    input: filename \n",
        "\n",
        "    output: dataframe after preprocessing\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file)\n",
        "    # Filter to get Noun, Verb, Object, Location/Organization, Person.\n",
        "    df.query('(posTag == \"Ny\") or (posTag == \"Np\") or (nerLabel == \"B-ORG\") or (nerLabel == \"B-LOC\") or (nerLabel == \"B-PER\") or (nerLabel == \"I-ORG\") or (nerLabel == \"I-LOC\") or (nerLabel == \"I-PER\") or ((posTag == \"N\" or posTag == \"A\" or posTag == \"V\") and nerLabel == \"O\")', inplace = True)\n",
        "    # Filter unicode\n",
        "    df = df.replace(r'[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ\\-_]', '', regex=True)\n",
        "    # Lower case\n",
        "    df[\"form\"] = df[\"form\"].str.lower()\n",
        "    return df\n",
        "\n",
        "\n",
        "def merge2Rows(df, prev, i, curRow):\n",
        "    \"\"\"\n",
        "    a sub-function for mergeFullNamePerson()\n",
        "    \"\"\"\n",
        "    prev[1].form = str(prev[1].form) + \"_\" + str(curRow.form)\n",
        "    df.at[prev[0], \"form\"] = prev[1].form\n",
        "def mergeFullNamePerson(file = r'/output/result_labeled.csv'):\n",
        "    \"\"\"\n",
        "    return a dataframe which gets fullname person by merging some rows which have firstname and lastname person\n",
        "    \"\"\"\n",
        "    df = preprocessing(file)\n",
        "    # Merge rows to get full-name person\n",
        "    for i, curRow in df.iterrows():\n",
        "        if (curRow.nerLabel == \"I-PER\"):\n",
        "            merge2Rows(df, prev, i, curRow) \n",
        "        else:\n",
        "            prev = [i, curRow]\n",
        "    # Query to get full-name person\n",
        "    df.query('(nerLabel == \"B-PER\")', inplace = True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def groupbyPersonLabel(file = r'/output/result_labeled.csv'):\n",
        "    \"\"\"\n",
        "    groupby 'form' and 'label' to show axactly this person and the lists of documents have them\n",
        "    \"\"\"\n",
        "    df = mergeFullNamePerson(file)\n",
        "    df.query('(nerLabel == \"B-PER\")', inplace = True)\n",
        "    df = df.groupby(['form','label']).document.apply(list).reset_index()\n",
        "    return df\n",
        "\n",
        "def getKeyw_doc(file = r'/output/result_labeled.csv'):\n",
        "    \n",
        "    df = pd.read_csv('input/processing/3_df_input_mergeFullNamePerson.csv')\n",
        "    df_all = pd.read_csv('input/processing/2_df_input_preprocessing.csv')\n",
        "    temp = []\n",
        "    description = []\n",
        "\n",
        "    for doc in range(df_all['document'].max()+1):\n",
        "        keywLO = []\n",
        "        keywP = []\n",
        "        keyw = []\n",
        "        key = []\n",
        "        df_key = df_all[df_all['document']==doc]\n",
        "        totalscore = 0\n",
        "        for m,n in df_key.iterrows():\n",
        "            if (n.nerLabel =='B-LOC' or n.nerLabel =='B-ORG'or n.nerLabel =='I-LOC' or n.nerLabel =='I-ORG'):\n",
        "                keywLO = keywLO + [n.form]\n",
        "                totalscore += 2\n",
        "            elif (n.nerLabel =='B-PER'or n.nerLabel=='I-PER'):\n",
        "                keywP = keywP + [n.form]\n",
        "                totalscore += 3\n",
        "            else:\n",
        "                keyw = keyw +[n.form]\n",
        "                totalscore += 1\n",
        "        my_dict = [{i:keywLO.count(i)*2/totalscore for i in keywLO},{i:keywP.count(i)*3/totalscore for i in keywP},{i:keyw.count(i)/totalscore for i in keyw}]\n",
        "        temp = temp + [my_dict]\n",
        "    keyw_in_doc = []\n",
        "    for i,j in df.iterrows():\n",
        "        keyw_in_doc = keyw_in_doc + [temp[j.document]]\n",
        "    df.insert(8,\"keywords\",keyw_in_doc)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSfpozgdal-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls\n",
        "# drive/  sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSAWNX5CXcx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processingDoc(folderInput = 'input/', folderOutput = 'output/'):\n",
        "  df = pd.DataFrame()\n",
        "  df = annotateDocument(folderInput, folderOutput)\n",
        "  if (df.empty):\n",
        "    print(\"Không có tên người\")\n",
        "    return\n",
        "  df.to_csv(\"input/processing/1_df_input.csv\", index = False, encoding='utf-8-sig')\n",
        "  df = preprocessing(file = r'input/processing/1_df_input.csv')\n",
        "  df.to_csv(\"input/processing/2_df_input_preprocessing.csv\", index = False, encoding='utf-8-sig')\n",
        "  df = mergeFullNamePerson(file = r'input/processing/1_df_input.csv')\n",
        "  df.to_csv(\"input/processing/3_df_input_mergeFullNamePerson.csv\", index = False, encoding='utf-8-sig')\n",
        "  df = getKeyw_doc()\n",
        "  df.loc[df.astype(str).drop_duplicates(subset=['document','form']).index].to_csv(\"input/processing/4_df_input_getKeyw.csv\", index = False, encoding='utf-8-sig')\n",
        "  return df\n",
        "df = processingDoc()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeTpCC0dCLtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls\n",
        "# drive/  sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvrCoJKSAQ4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from collections import Counter \n",
        "import ast\n",
        "import math\n",
        "\n",
        "\n",
        "def score(df_train, df_test_each_person):\n",
        "    test_person = df_test_each_person.form\n",
        "    test_keywords = ast.literal_eval(df_test_each_person.keywords)\n",
        "    test_keywords = dict(Counter(test_keywords[0]) + Counter(test_keywords[1]) + Counter(test_keywords[2]))\n",
        "    # print(df_train)\n",
        "\n",
        "    result = np.zeros(int(df_train.shape[0]))\n",
        "\n",
        "    for i, row in df_train.iterrows():\n",
        "        train_alias = row.alias.replace('[', '').replace(']', '').split(\",\")\n",
        "        train_keywords = ast.literal_eval(row.keywords)\n",
        "        total = 0\n",
        "        if(train_keywords[0]):\n",
        "            total += sum(train_keywords[0].values())*2\n",
        "        if(train_keywords[1]):\n",
        "            total += sum(train_keywords[1].values())*3\n",
        "        if(train_keywords[2]):\n",
        "            total += sum(train_keywords[2].values())\n",
        "        # print(total)\n",
        "        per = 0\n",
        "        if test_person in train_alias:\n",
        "            # result[i] += 1\n",
        "            # print(test_person)\n",
        "\n",
        "            per += 1\n",
        "        for test_keyword in test_keywords.keys():\n",
        "            result[i] += int(Counter(train_keywords[0])[test_keyword])*test_keywords[test_keyword] \n",
        "            result[i] += int(Counter(train_keywords[1])[test_keyword])*test_keywords[test_keyword] \n",
        "            result[i] += int(Counter(train_keywords[2])[test_keyword])*test_keywords[test_keyword]\n",
        "        # print(per)\n",
        "        result[i]=(result[i]/total)*math.exp(per)\n",
        "    # print(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "def classifyDoc():\n",
        "    attribute = []\n",
        "    df_train = pd.read_csv('PerfectProfile.csv')\n",
        "    df_test = pd.read_csv('input/processing/4_df_input_getKeyw.csv')\n",
        "\n",
        "    for i in range(0, df_train.shape[0] + 1): # nRow in df_train\n",
        "        eachDoc = df_test.query(str('(document == ' + str(i) + ')'), inplace = False)\n",
        "        print(\"Document \" + str(i) + \": \")\n",
        "        for iPerson, j in eachDoc.iterrows():\n",
        "            scorePerson = score(df_train, j) \n",
        "            predictPerson = df_train.loc[np.where(scorePerson == np.amax(scorePerson))] # a row in df_train\n",
        "            print(\"Person: \" + j.form)\n",
        "            print(\"Score: \")\n",
        "            print(scorePerson) # array\n",
        "            print(\"Predict: \")\n",
        "            print(\"Perdict person: \" + str(predictPerson.name.values[0]))\n",
        "            print(\"Description: \" + str(predictPerson.description.values[0]))\n",
        "            print()\n",
        "        print(\"-------------------------\")\n",
        "\n",
        "classifyDoc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXrHikxzKjdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_d(d):\n",
        "  d0 = Counter()\n",
        "  d1 = Counter()\n",
        "  d2 = Counter()\n",
        "\n",
        "  for doc in d:\n",
        "    d0 = d0 + Counter(doc[0])\n",
        "    d1 = d1 + Counter(doc[1])\n",
        "    d2 = d2 + Counter(doc[2])\n",
        "  return [dict(d0),dict(d1),dict(d2)]\n",
        "\n",
        "def getKeyw(file = r'output/result_labeled.csv'):\n",
        "  df = groupbyPersonLabel(file)\n",
        "  df_all = preprocessing(file)\n",
        "  temp = []\n",
        "  description = []\n",
        "  # i = 0\n",
        "  for i,j in df.iterrows():\n",
        "    # temp[j.form]=pd.DataFrame()\n",
        "    keywLO = []\n",
        "    keywP = []\n",
        "    keyw = []\n",
        "    key = []\n",
        "\n",
        "\n",
        "    # i = i + 1\n",
        "    # if (i == 10):\n",
        "    #     break\n",
        "\n",
        "\n",
        "    for doc in j.document:\n",
        "\n",
        "      df_key = df_all[df_all['document']==doc]\n",
        "      for m,n in df_key.iterrows():\n",
        "        if (n.nerLabel =='B-LOC' or n.nerLabel =='B-ORG'or n.nerLabel =='I-LOC' or n.nerLabel =='I-ORG'):\n",
        "          keywLO = keywLO + [n.form]\n",
        "        elif (n.nerLabel =='B-PER'or n.nerLabel=='I-PER'):\n",
        "          keywP = keywP + [n.form]\n",
        "        else:\n",
        "          keyw = keyw +[n.form]\n",
        "      my_dict = [{i:keywLO.count(i) for i in keywLO},{i:keywP.count(i) for i in keywP},{i:keyw.count(i) for i in keyw}]\n",
        "      key = key +[my_dict]\n",
        "    temp = temp + [key]\n",
        "    \n",
        "    description = description +['Person number: ' + str(i)]\n",
        "  df['keywords'] = [flatten_d(x) for x in temp]\n",
        "  df['description']=description\n",
        "  # print(df)\n",
        "  return df\n",
        "# df1 = pd.DataFrame()\n",
        "# df1 = getKeyw(\"output/result_labeled.csv\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}